{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f284067c",
   "metadata": {},
   "source": [
    "# Automatic Results Analysis \n",
    "\n",
    "This notebook serves as a starting point to analyze your results using the pipeline. It can be extended for your needs or highly modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8273ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1400b",
   "metadata": {},
   "source": [
    "Use this following notebook cell to input the path to the model you want to analyze results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7267bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"model_weights/efficientnet/b5/no_wandb_20250807_103340\"  # Replace with your model path\n",
    "TRAINING_LOG_PATH = f\"{MODEL_DIR}/training_log.csv\"\n",
    "CLASSIF_REPORT = f\"classif_report_best_efficientnet.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500d222",
   "metadata": {},
   "source": [
    "## Validation & Train loss over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7650a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAINING_LOG_PATH, comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b55d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss')\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.title('Training and Validation Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257396f",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top1 and Top5 Accuracy over epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['epoch'], df['top1_acc'], label='Top-1 Accuracy')\n",
    "plt.plot(df['epoch'], df['top5_acc'], label='Top-5 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Top-1 and Top-5 Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9836f03",
   "metadata": {},
   "source": [
    "## Metrics over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a graph for metrics over epochs \n",
    "# Top1 and Top5 Accuracy over epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(df['epoch'], df['f1_macro'],  label='F1 Macro')\n",
    "plt.plot(df['epoch'], df['recall_macro'], label='Recall Macro')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Performance Metrics')\n",
    "plt.title(' Performance Metrics over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786c16a",
   "metadata": {},
   "source": [
    "## F1-Score vs Class Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1e1da",
   "metadata": {},
   "source": [
    "We are working with the generated classification report from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72132575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_classif = pd.read_csv(CLASSIF_REPORT)\n",
    "# Filter out summary rows (if they exist)\n",
    "df_report_classif_cleaned = df_report_classif[~df_report_classif['class'].isin(['accuracy', 'macro avg', 'weighted avg'])].copy()\n",
    "# Convert to numeric, handling any non-numeric values\n",
    "df_report_classif_cleaned['f1-score'] = pd.to_numeric(df_report_classif_cleaned['f1-score'], errors='coerce')\n",
    "df_report_classif_cleaned['support'] = pd.to_numeric(df_report_classif_cleaned['support'], errors='coerce')\n",
    "# Drop rows with NaN values in case there are any (shouldn't be the case)\n",
    "df_report_classif_cleaned.dropna(subset=['f1-score', 'support'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791de83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1-Score vs Support\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(df_report_classif_cleaned['support'], df_report_classif_cleaned['f1-score'], color='tab:blue', edgecolor='k', s=70, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Support (Number of Samples)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Support')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f0b39",
   "metadata": {},
   "source": [
    "## Worst Classes and Best Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da350f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap: single column with model name\n",
    "model_name = \"My Super AI Model\"  # Change this to your actual model name if needed\n",
    "bottom_n = 10\n",
    "top_n = 10\n",
    "\n",
    "# Select the bottom N classes by F1-score\n",
    "worst_classes = df_report_classif_cleaned.nsmallest(bottom_n, 'f1-score')\n",
    "top_classes = df_report_classif_cleaned.nlargest(top_n, 'f1-score')\n",
    "worst_classes['model'] = model_name\n",
    "top_classes['model'] = model_name\n",
    "\n",
    "# Pivot for heatmaps\n",
    "pivot_df_top = top_classes.pivot(index='class', columns='model', values='f1-score')\n",
    "pivot_df_worst = worst_classes.pivot(index='class', columns='model', values='f1-score')\n",
    "\n",
    "# Determine figure height based on max rows\n",
    "n_rows = max(len(pivot_df_worst), len(pivot_df_top))\n",
    "fig_height = max(6, n_rows * 0.4)\n",
    "\n",
    "# Plot side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, fig_height), sharex=False)\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_df_worst,\n",
    "    annot=True,\n",
    "    cmap='coolwarm_r',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    cbar_kws={'label': 'F1-score'},\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(f'Worst {bottom_n} Classes')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Class')\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_df_top,\n",
    "    annot=True,\n",
    "    cmap='coolwarm_r',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    cbar_kws={'label': 'F1-score'},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(f'Top {top_n} Classes')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('')  # Hide redundant y-label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d6840",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5083ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_MATRIX_CSV = f\"{MODEL_DIR}/confusion_matrix_raw.csv\"\n",
    "CONF_MATRIX_RAW_IMG = f\"{MODEL_DIR}/confusion_matrix_raw.png\"\n",
    "CONF_MATRIX_NORM_IMG = f\"{MODEL_DIR}/confusion_matrix_normalized.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a8ce274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Confusion Matrix\n",
      "Raw confusion matrix saved at: model_weights/efficientnet/b5/no_wandb_20250807_103340/confusion_matrix_raw.png\n",
      "Normalized confusion matrix saved at: model_weights/efficientnet/b5/no_wandb_20250807_103340/confusion_matrix_normalized.png\n"
     ]
    }
   ],
   "source": [
    "# Display the confusion matrices\n",
    "print(\"# Confusion Matrix\")\n",
    "print(f\"Raw confusion matrix saved at: {CONF_MATRIX_RAW_IMG}\")\n",
    "print(f\"Normalized confusion matrix saved at: {CONF_MATRIX_NORM_IMG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e983a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display the confusion matrices\n",
    "display(Image(filename=CONF_MATRIX_RAW_IMG))\n",
    "display(Image(filename=CONF_MATRIX_NORM_IMG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe6710",
   "metadata": {},
   "source": [
    "Let's check which classes are the most confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "top_n = 10\n",
    "\n",
    "# Load confusion matrix\n",
    "cm_df = pd.read_csv(CONF_MATRIX_CSV, index_col=0)\n",
    "\n",
    "# Flatten confusion matrix (excluding diagonal)\n",
    "confusions = []\n",
    "for true_class in cm_df.index:\n",
    "    for pred_class in cm_df.columns:\n",
    "        if true_class != pred_class:\n",
    "            count = cm_df.loc[true_class, pred_class]\n",
    "            confusions.append((true_class, pred_class, count))\n",
    "\n",
    "# Create DataFrame from confusion data\n",
    "confusion_df = pd.DataFrame(confusions, columns=['True Class', 'Predicted Class', 'Count'])\n",
    "\n",
    "# Sort by most confused\n",
    "top_confusions = confusion_df.sort_values(by='Count', ascending=False).head(top_n)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, max(4, top_n * 0.5)))\n",
    "sns.barplot(\n",
    "    data=top_confusions,\n",
    "    x='Count',\n",
    "    y=top_confusions.apply(lambda row: f\"{row['True Class']} → {row['Predicted Class']}\", axis=1),\n",
    "    palette='Reds_r'\n",
    ")\n",
    "plt.xlabel(\"Number of Confusions\")\n",
    "plt.ylabel(\"Class Confusion (True → Predicted)\")\n",
    "plt.title(f\"Top {top_n} Most Confused Class Pairs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
