# ============================================================
# ğŸ¯ EfficientNet Hyperparameter Sweep - BEGINNER FRIENDLY
# ============================================================

# ğŸ“– WHAT IS A HYPERPARAMETER SWEEP?
# A sweep automatically trains multiple models with different settings to find the best combination.
# Instead of manually trying different learning rates, optimizers, etc., the sweep does it automatically.
# This helps you find the optimal settings for your specific dataset without guesswork.

# ğŸ” SEARCH METHOD
method: bayes                   # ğŸ§  Bayesian optimization - smart search algorithm
                                # "bayes" = learns from previous results to make better guesses
                                # "random" = tries random combinations (faster but less efficient)
                                # "grid" = tries every combination (thorough but very slow)

# ğŸ¯ OPTIMIZATION TARGET
metric:
  goal: maximize                # ğŸ“ˆ We want to maximize this metric
  name: val/metrics/f1_macro    # ğŸ† F1 macro score (balanced accuracy across all classes)
                                # This metric works well for most classification tasks
                                # Alternative: "val/metrics/accuracy_top1" for simple accuracy

# ğŸ”§ PARAMETERS TO OPTIMIZE
parameters:
  
  # ğŸ¤– MODEL ARCHITECTURE SETTINGS
  model_name:
    values: ["efficientnet"]    # ğŸ§  Only testing EfficientNet in this sweep
  
  efficientnet_variant:
    values: ['b2', 'b3', 'b4', 'b5']  # ğŸ“ Test different EfficientNet sizes
                                       # b2 = faster, less accurate
                                       # b5 = slower, more accurate
                                       # Sweep will find the best balance for your data
  
  # ğŸ“‰ LOSS FUNCTION SETTINGS
  loss_type:
    values: ["weighted", "focal", "labelsmoothing"]  # ğŸ¯ Different loss functions
                                                     # "weighted" = handles class imbalance automatically
                                                     # "focal" = focuses on hard-to-classify examples
                                                     # "labelsmoothing" = prevents overconfidence
  
  # ğŸ”§ FOCAL LOSS PARAMETERS (only used when loss_type = "focal")
  focal_alpha:
    min: 0.1                    # âš–ï¸ Balance between positive/negative examples
    max: 0.5                    # Lower = focus on negative examples, Higher = focus on positive
  
  focal_gamma:
    min: 1.0                    # ğŸ¯ How much to focus on hard examples
    max: 3.0                    # Higher = focus more on difficult cases
  
  # âš–ï¸ CLASS BALANCING
  use_per_class_alpha:
    values: [true, false]       # Whether to weight classes differently
                                # true = give more weight to underrepresented classes
                                # false = treat all classes equally
  
  # ğŸšï¸ LABEL SMOOTHING PARAMETER (only used when loss_type = "labelsmoothing")
  labelsmoothing_epsilon:
    min: 0.05                   # ğŸ“Š Amount of label smoothing
    max: 0.2                    # Higher = more smoothing, less overconfidence
  
  # ğŸ‹ï¸ TRAINING SETTINGS
  epochs:
    value: 30                   # ğŸ”„ Fixed number of training epochs for all runs
                                # Enough to see clear results without taking too long
  
  batch_size:
    values: 64                  # ğŸ”¢ Fixed batch size
                                # You can add more values like [32, 64, 128] to test different sizes
  
  optimizer:
    values: ["adamw", "adam"]   # ğŸ¯ Test different optimization algorithms
                                # Both are good choices - sweep will find which works better
  
  # ğŸ“ˆ LEARNING RATE SEARCH (very important!)
  learning_rate:
    values: [1e-7, 5e-7,1e-6, 5e-6,1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 2e-1]
    # ğŸ” Testing a wide range from very small (1e-7) to large (2e-1)
    # The sweep will find the sweet spot for your data
  
  # ğŸ›¡ï¸ WEIGHT DECAY SEARCH (regularization)
  weight_decay:
    values: [1e-7, 5e-7,1e-6, 5e-6,1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 2e-1]
    # ğŸ” Testing different regularization strengths
    # Higher values prevent overfitting but might hurt performance
  
  # â¹ï¸ FIXED TRAINING SETTINGS
  early_stopping_patience:
    value: 15                   # Stop training if no improvement for 15 epochs
    
  device:
    value: "cuda"               # ğŸ’» Use GPU for all sweep runs (much faster)

# ğŸ’¡ HOW TO USE THIS SWEEP:
# 1. Make sure you have Weights & Biases account and are logged in
# 2. Run: python3 run_sweep.py --sweep_config configs/sweeps/sweep_efficientnet.yaml
# 3. The sweep will automatically run multiple training sessions
# 4. Check W&B dashboard to see results and find the best configuration
# 5. Use the best settings for your final training run

# ğŸ¯ WHAT TO EXPECT:
# - The sweep will run 20-50 different combinations automatically
# - Each run takes the same time as normal training (30 epochs)
# - Total time: several hours to a day depending on your dataset
# - Results: optimal hyperparameters for your specific data

# âš ï¸ BEGINNER WARNINGS:
# - Sweeps use lots of computational resources - make sure you have time/budget
# - Start with a smaller sweep first (fewer values) to test
# - Monitor the first few runs to ensure everything works correctly
# - Consider running during off-hours due to long duration

# ğŸ”§ CUSTOMIZATION FOR BEGINNERS:
# To make this sweep faster for testing:
# 1. Reduce efficientnet_variant to just ['b2', 'b3']
# 2. Reduce learning_rate values to [1e-6, 1e-5, 1e-4, 1e-3]
# 3. Reduce weight_decay values to [1e-5, 1e-4, 1e-3, 1e-2]
# 4. Set epochs to 15 instead of 30 for faster testing
