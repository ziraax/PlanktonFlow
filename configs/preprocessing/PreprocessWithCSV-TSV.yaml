# ============================================================
# 📑 CSV/TSV DATASET PREPROCESSING - BEGINNER FRIENDLY
# ============================================================

# 📖 WHAT IS CSV/TSV DATA FORMAT?
# This is when you have:
# 1. A folder with all your images (all mixed together, not in species folders)
# 2. A CSV or TSV file that tells you which species each image belongs to
#
# Example structure:
# YourDataset/
# ├── images/
# │   ├── img001.jpg
# │   ├── img002.jpg
# │   ├── img003.jpg
# │   └── ...
# └── labels.csv  (contains: filename,species)
#     img001.jpg,species_A
#     img002.jpg,species_B
#     img003.jpg,species_A

# 📂 INPUT DATA CONFIGURATION
input_source:
  type: "csv_mapping"                                      # Tell system this is CSV/TSV mapped data
  
  images_path: "DATA/YourCSV-TSVDataset/images"           # 📁 Folder containing ALL your images
                                                          # All images should be in this one folder
                                                          
  metadata_file: "DATA/YourCSV-TSVDataset/image_labels.csv"  # 📄 CSV/TSV file with image-to-species mapping
                                                              # Each row should have: image_filename,species_name
                                                              
  image_column: "filename"                                 # 🔤 Column name that contains image filenames
                                                          # Must match exactly what's in your CSV header
                                                          # Common names: "filename", "image", "file", "image_name"
                                                          
  label_column: "species"                                 # 🏷️ Column name that contains species/class labels
                                                          # Must match exactly what's in your CSV header  
                                                          # Common names: "species", "label", "class", "category"
                                                          
  separator: ","                                          # 📊 Character that separates columns in your file
                                                          # "," = CSV (comma-separated values)
                                                          # "\t" = TSV (tab-separated values)
                                                          # ";" = semicolon-separated (sometimes used in Europe)
                                                          
  image_path_prefix: ""                                   # 📍 Extra path prefix if needed (usually leave empty)
                                                          # Only needed if CSV contains partial paths
                                                          # Example: if CSV has "subdir/img.jpg", leave empty
                                                          # If CSV has just "img.jpg" but images are in "photos/img.jpg", use "photos/"

# 🔧 PREPROCESSING SETTINGS (same as hierarchical, see detailed comments there)
preprocessing:
  
  # 🏗️ SCALEBAR REMOVAL
  scalebar_removal:
    enabled: true               # 🔍 Whether to detect and remove scale bars from images
                                # true = remove scale bars (RECOMMENDED for microscopy images)
                                # false = leave images unchanged
                                
    model_path: "models/model_weights/scale_bar_remover/best.pt"  # 🤖 Pre-trained scale bar detection model
                                                                  # Don't change this path - it's the trained model
                                                                  
    confidence: 0.4             # 🎯 How confident the model needs to be to detect a scale bar (0.0-1.0)
                                # 0.4 = good balance, detects most scale bars without false positives
                                # Lower = detects more scale bars but may make mistakes
                                # Higher = more careful but might miss some scale bars
                                
    img_size: 416               # 📐 Image size for scale bar detection model
                                # Keep this at 416 - it's optimized for the detection model
  
  # 🎨 COLOR CONVERSION
  grayscale_conversion:
    enabled: true               # 🌈 Convert grayscale images to RGB format
                                # true = ensure all images have color channels (RECOMMENDED)
                                # false = keep original format (only if all images are already RGB)
    mode: "RGB"                 # Color mode - keep as "RGB"
  
  # 🧹 IMAGE FILTERING (removes bad/insufficient data)
  image_filtering:
    min_images_per_class: 10    # 📊 Remove classes with fewer than this many images
                                # Classes with too few images can't be learned properly
                                # 10 is minimum, 50+ is better for good results
                                
    max_images_per_class: null  # 📏 Maximum images per class (null = no limit)
                                # Set a number (e.g., 1000) if some classes have too many images
                                # null = keep all images
                                
    skip_corrupted_images: true # 🚫 Skip images that can't be opened
                                # true = ignore broken image files (RECOMMENDED)
  
  # 📊 DATA SPLITTING (divide data into training/validation/test sets)
  data_splitting:
    train_ratio: 0.7            # 🏋️ Fraction of data for training (0.7 = 70%)
                                # This is where the model learns patterns
                                
    val_ratio: 0.2              # 📏 Fraction for validation (0.2 = 20%)
                                # Used to monitor training progress and tune settings
                                
    test_ratio: 0.1             # 🧪 Fraction for final testing (0.1 = 10%)
                                # Used to evaluate final model performance
                                # train_ratio + val_ratio + test_ratio must equal 1.0
                                
    stratified: true            # ⚖️ Keep class proportions balanced across splits
                                # true = each split has similar class distribution (RECOMMENDED)
                                
    random_seed: 42             # 🎲 Random seed for reproducible splits
                                # Same seed = same splits every time you run preprocessing
  
  # 🔄 DATA AUGMENTATION (creates more training examples)
  augmentation:
    enabled: true               # 🎨 Create additional training images through transformations
                                # true = more training data, better model performance (RECOMMENDED)
                                # false = use only original images
                                
    target_images_per_class: 30 # 🎯 Target number of images per class after augmentation
                                # Smaller datasets: try 50-200
                                # Larger datasets: try 500-2000
                                # Adjust based on your computational resources
                                
    max_copies_per_image: 3     # 📸 Maximum augmented versions per original image
                                # Higher = more variety but also more processing time
                                # 3-5 is usually a good range

    # 🎭 AUGMENTATION TECHNIQUES (how to modify images)
    # Each number is the probability (0.0-1.0) of applying that transformation
    techniques:
      horizontal_flip: 0.5      # 🔄 Flip image left-right (50% chance)
                                # Good for most natural images
                                
      vertical_flip: 0.2        # ↕️ Flip image top-bottom (20% chance)
                                # Lower probability - not all images make sense flipped vertically
                                
      rotate_90: 0.3            # 🔄 Rotate by 90 degrees (30% chance)
                                # Good for microscopy images that can be oriented any way
                                
      brightness_contrast: 0.4  # ☀️ Adjust brightness and contrast (40% chance)
                                # Helps model handle different lighting conditions
                                
      hue_saturation: 0.3       # 🌈 Adjust color properties (30% chance)
                                # Helps with color variations in images
      
# 📁 OUTPUT CONFIGURATION (where to save processed data)
output:
  # Base path for output files
  base_path: "DATA/YourEcoTaxaDataset"
  # Processed path where files with no scale bars will be saved
  processed_path: "DATA/YourEcoTaxaDataset_processed"
  # Final dataset path where the final dataset will be saved
  final_dataset_path: "DATA/YourEcoTaxaDataset_final_dataset"
  # Unless you already have a dataset YAML file correctly formatted, please always set this to true since its needed for training
  create_dataset_yaml: true
  
# 📊 LOGGING CONFIGURATION (track what happens during preprocessing)
logging:
  wandb_enabled: false          # 🌐 Upload preprocessing logs to Weights & Biases
                                # false = keep logs local only (RECOMMENDED for preprocessing)
                                # true = upload statistics and charts online

  # These features are not yet implemented                   
  log_class_distribution: true  # 📊 Show how many images per class
                                # true = see statistics about your data balance             
  log_sample_images: false      # 🖼️ Save example processed images
                                # false = faster processing (RECOMMENDED)
                                # true = save examples to see what preprocessing did           
  log_processing_times: true    # ⏱️ Track how long each step takes
                                # true = see performance statistics

# 💡 QUICK START GUIDE:
# 1. Change data_path to point to YOUR image folders
# 2. Adjust target_images_per_class based on your dataset size:
#    - Small dataset (< 1000 images): try 100-300
#    - Medium dataset (1000-10000 images): try 500-1000  
#    - Large dataset (> 10000 images): try 1000-3000
# 3. Run: python3 run_preprocessing.py --config configs/preprocessing/PreprocessWithHierarchical.yaml
# 4. Use the final_dataset_path folder for training!

# 🚨 COMMON ISSUES:
# - "No classes found": Check that data_path points to folder containing species folders
# - "Not enough images": Reduce min_images_per_class or get more data
# - "Out of memory": Reduce target_images_per_class or max_copies_per_image
